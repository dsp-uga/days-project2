from __future__ import print_function
import urllib

from pyspark.sql import SQLContext, Row,SparkSession
from pyspark.ml.feature import IDF, Tokenizer,HashingTF
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.types import *
from pyspark.ml.feature import CountVectorizer
from pyspark.sql.functions import lit
from pyspark.ml.feature import NGram


import re
import os

#warehouseLocation = 'file:///home/dharamendra/PycharmProjects/MalwareDetection'
#sc = SparkContext(conf=SparkConf().setAppName("MalwareClassifier"))
spark = SparkSession\
        .builder\
        .appName("MalwareClassification")\
        .getOrCreate()
sc=spark.sparkContext;
sqlContext=SQLContext(sc)

#######Method for Preprocessing
def uniqueByte(inputByte):
    x= list(set(inputByte))
    temp=[]
    for str in x:
        tempStr=str.replace(" ","")
        temp.append(tempStr.encode("utf-8"))
    return temp

########Main method for Main
def main():
    fields = [StructField("hashcodefile", StringType(), True), StructField("label", StringType(), True),
              StructField("n-grams", ArrayType(StringType(), True), True)]
    schema = StructType(fields)
    schemaByteParque=spark.read.parquet("cleanFile.parquet")
    #Reading Testing data set

    #schemaByteTestParque=schemaByteTestParqueTemp.withColumn("label",lit("").cast(StringType())).select("label","features")

    print ("Parquet File read completed")
    #schemaByteParque.createOrReplaceTempView("byteDataFrame")

    ngram = NGram(n=2, inputCol="content", outputCol="n-grams")
    #Creating Testing N-Gram
    ngramDataFrame = ngram.transform(schemaByteParque).select("hashcodefile","label","n-grams")
    ngramRDD = ngramDataFrame.rdd
    #ngramRDD.saveAsTextFile("/home/dharamendra/7.txt")

    #Creating Testing dataset n-gram dataframe

    print ("N-gram completed for testing & training")

    #ngramRDDUnique = ngramRDD.map(lambda line: (line[0].encode('utf-8'),line[1], uniqueByte(line[2])))
    #ngramRDDUnique.saveAsTextFile("/home/dharamendra/unique.txt")
    #Creating unique features of testing dataset
    print ("N-Gram unique completed")
    #ngramDataFrameStringRDD=ngramRDD.map(lambda line:(line[0],line[1].encode('utf-8'),' '.join(str(x) for x in line[2])))

    schemaByteTestParqueTemp = spark.read.parquet("cleanTestFile.parquet")
    ngramTestData = NGram(n=2, inputCol="features", outputCol="n-grams")
    ngramTestDataFrame = ngramTestData.transform(schemaByteTestParqueTemp).select("hashcodefile","label","n-grams")
    ngramTestDataRDD=ngramTestDataFrame.rdd
    #ngramTestDataUniqueRDD = ngramTestDataRDD.map(lambda line: (line[0].encode('utf-8'), uniqueByte(line[1])))
    #ngramTestDataStringRDD=ngramTestDataRDD.map(lambda line:(line[0],line[1].encode('utf-8'),' '.join(str(x) for x in line[2])))

    # print ("Array type to String Type conversion completed")
    # #Conversion of RDD to Dataframe
    inputNgram=spark.createDataFrame(ngramRDD,schema)

    inputTestNgram = spark.createDataFrame(ngramTestDataRDD, schema)

    # #count=ngramRDDUnique.flatMap(lambda line:(' '.join(str(x) for x in line[2])).split(" ")).distinct().count()
    # #print ("Count:",count)
    # #count.saveAsTextFile("file:///home/dharamendra/unique.txt");


    # # #==============================================================================
    # # #Term Frequency for training data set
    # # #=====================================================

    #tokenizer = Tokenizer(inputCol="2-grams", outputCol="bytes")
    #inputData = tokenizer.transform(inputTFDF).select("label","bytes")
    #inputData.show(2)
    cv = CountVectorizer(inputCol="n-grams", outputCol="features", vocabSize=65536, minDF=1.0,minTF=2.0)
    model = cv.fit(inputNgram)
    featurizedData = model.transform(inputNgram).select("label","features")
    featurizedData.show()
    print ("Term Frequency completed for training data set")

    # # ###############################
    # # ## Term frequency for testing data set
    # # ####################
    #tokenizerTestData = Tokenizer(inputCol="2-grams", outputCol="bytes")
    #inputTestData = tokenizerTestData.transform(inputTestNgram)
    cvTest = CountVectorizer(inputCol="n-grams", outputCol="features", vocabSize=65536, minDF=1.0,minTF=2.0)
    modelTest = cvTest.fit(inputTestNgram)
    featurizedTestData = modelTest.transform(inputTestNgram).select("label","features")
    featurizedTestData.show()
    print("Term Frequency completed for testing data set")

    # ######## Code for Random Forest Classifier
    labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(featurizedData)

    #featureIndexer = \
    #     VectorIndexer(inputCol="features", outputCol="indexedFeatures",maxCategories=10).fit(featurizedData)


    # # # # Split the data into training and test sets (30% held out for testing)
    # (trainingData, testData) = featurizedData.randomSplit([0.7, 0.3])
    # testData.show()

    # # Train a RandomForest model.
    rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="features", numTrees=26,maxDepth=8,maxBins=10240,impurity="entropy")

    #  Convert indexed labels back to original labels.
    labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                                    labels=labelIndexer.labels)

    # # Chain indexers and forest in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer,rf,labelConverter])
    #
    # # Train model.  This also runs the indexers.
    model = pipeline.fit(featurizedData)

    # #
    # # # Make predictions.
    predictions = model.transform(featurizedTestData)

    # #
    # # # Select example rows to display.
    filterPredictions=predictions.select("label","prediction","predictedLabel","indexedLabel")
    filterPredictions.show(3)
    predictionsRDD=filterPredictions.rdd
    predictionsRDD.saveAsTextFile("/home/dharamendra/rf4.txt")
    # #Select (prediction, true label) and compute test error
    evaluator = MulticlassClassificationEvaluator(
         labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(filterPredictions)
    print("Test Error = %g" % (1.0 - accuracy))
    #
    rfModel = model.stages[1]
    print(rfModel)
    #============================================================
if __name__ == "__main__":
    main()
